INFO 03-24 19:58:47 [cache_tensor_manager.py:17] USE_GPU_TENSOR_CACHE is On
INFO 03-24 19:58:53 __init__.py:190] Automatically detected platform cuda.
INFO 03-24 19:59:05 [cache_tensor_manager.py:17] USE_GPU_TENSOR_CACHE is On
INFO 03-24 19:59:05 [cache_tensor_manager.py:17] USE_GPU_TENSOR_CACHE is On
INFO 03-24 19:59:05 [cache_tensor_manager.py:17] USE_GPU_TENSOR_CACHE is On
INFO 03-24 19:59:05 [cache_tensor_manager.py:17] USE_GPU_TENSOR_CACHE is On
INFO 03-24 19:59:06 [cache_tensor_manager.py:17] USE_GPU_TENSOR_CACHE is On
INFO 03-24 19:59:06 [cache_tensor_manager.py:17] USE_GPU_TENSOR_CACHE is On
INFO 03-24 19:59:06 [cache_tensor_manager.py:17] USE_GPU_TENSOR_CACHE is On
INFO 03-24 19:59:06 [cache_tensor_manager.py:17] USE_GPU_TENSOR_CACHE is On
INFO 03-24 19:59:13 __init__.py:190] Automatically detected platform cuda.
INFO 03-24 19:59:13 __init__.py:190] Automatically detected platform cuda.
INFO 03-24 19:59:13 __init__.py:190] Automatically detected platform cuda.
INFO 03-24 19:59:14 __init__.py:190] Automatically detected platform cuda.
INFO 03-24 19:59:14 __init__.py:190] Automatically detected platform cuda.
INFO 03-24 19:59:14 __init__.py:190] Automatically detected platform cuda.
INFO 03-24 19:59:14 __init__.py:190] Automatically detected platform cuda.
INFO 03-24 19:59:14 __init__.py:190] Automatically detected platform cuda.
INFO 03-24 20:00:00 [__init__.py:46] select fp8w8a8-b128 quant way: deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [__init__.py:46] select fp8w8a8-b128 quant way: deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [basemodel.py:119] Initial quantization. The default quantization method is deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [basemodel.py:119] Initial quantization. The default quantization method is deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [__init__.py:46] select fp8w8a8-b128 quant way: deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [__init__.py:46] select fp8w8a8-b128 quant way: deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [__init__.py:46] select fp8w8a8-b128 quant way: deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [basemodel.py:119] Initial quantization. The default quantization method is deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [basemodel.py:119] Initial quantization. The default quantization method is deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [__init__.py:46] select fp8w8a8-b128 quant way: deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [__init__.py:46] select fp8w8a8-b128 quant way: deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [basemodel.py:119] Initial quantization. The default quantization method is deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [basemodel.py:119] Initial quantization. The default quantization method is deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [basemodel.py:119] Initial quantization. The default quantization method is deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [__init__.py:46] select fp8w8a8-b128 quant way: deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [basemodel.py:119] Initial quantization. The default quantization method is deepgemm-fp8w8a8-b128
INFO 03-24 20:00:00 [shared_arr.py:17] create shm None_mem_manger_can_use_token_num_7
INFO 03-24 20:00:00 [shared_arr.py:17] create shm None_mem_manger_can_use_token_num_6
INFO 03-24 20:00:00 [shared_arr.py:17] create shm None_mem_manger_can_use_token_num_0
INFO 03-24 20:00:00 [shared_arr.py:17] create shm None_mem_manger_can_use_token_num_5
INFO 03-24 20:00:00 [shared_arr.py:17] create shm None_mem_manger_can_use_token_num_4
INFO 03-24 20:00:00 [shared_arr.py:17] create shm None_mem_manger_can_use_token_num_1
INFO 03-24 20:00:00 [shared_arr.py:17] create shm None_mem_manger_can_use_token_num_2
INFO 03-24 20:00:00 [shared_arr.py:17] create shm None_mem_manger_can_use_token_num_3
INFO 03-24 20:01:06 [cuda_graph.py:130] Begin capture cudagraph, use the --disable_cudagraph to disable it.
INFO 03-24 20:01:16 [cuda_graph.py:130] Begin capture cudagraph, use the --disable_cudagraph to disable it.
INFO 03-24 20:01:17 [cuda_graph.py:130] Begin capture cudagraph, use the --disable_cudagraph to disable it.
INFO 03-24 20:01:17 [cuda_graph.py:130] Begin capture cudagraph, use the --disable_cudagraph to disable it.
INFO 03-24 20:01:18 [cuda_graph.py:130] Begin capture cudagraph, use the --disable_cudagraph to disable it.
INFO 03-24 20:01:18 [cuda_graph.py:130] Begin capture cudagraph, use the --disable_cudagraph to disable it.
INFO 03-24 20:01:18 [cuda_graph.py:130] Begin capture cudagraph, use the --disable_cudagraph to disable it.
INFO 03-24 20:01:20 [cuda_graph.py:130] Begin capture cudagraph, use the --disable_cudagraph to disable it.
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/moe_silu_and_mul_kernel/{N=2048,out_dtype=torch.bfloat16}_NVIDIA_H200.json kernel name moe_silu_and_mul_kernel use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/moe_silu_and_mul_kernel/{N=2048,out_dtype=torch.bfloat16}_NVIDIA_H200.json kernel name moe_silu_and_mul_kernel use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/moe_silu_and_mul_kernel/{N=2048,out_dtype=torch.bfloat16}_NVIDIA_H200.json kernel name moe_silu_and_mul_kernel use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/moe_silu_and_mul_kernel/{N=2048,out_dtype=torch.bfloat16}_NVIDIA_H200.json kernel name moe_silu_and_mul_kernel use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/moe_silu_and_mul_kernel/{N=2048,out_dtype=torch.bfloat16}_NVIDIA_H200.json kernel name moe_silu_and_mul_kernel use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/moe_silu_and_mul_kernel/{N=2048,out_dtype=torch.bfloat16}_NVIDIA_H200.json kernel name moe_silu_and_mul_kernel use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/moe_silu_and_mul_kernel/{N=2048,out_dtype=torch.bfloat16}_NVIDIA_H200.json kernel name moe_silu_and_mul_kernel use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/moe_silu_and_mul_kernel/{N=2048,out_dtype=torch.bfloat16}_NVIDIA_H200.json kernel name moe_silu_and_mul_kernel use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/mla_decode_attentnion/{out_dtype=torch.bfloat16,q_head_dim=512,q_head_num=128,q_rope_dim=64}_NVIDIA_H200.json kernel name mla_decode_attentnion use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/mla_decode_attentnion/{out_dtype=torch.bfloat16,q_head_dim=512,q_head_num=128,q_rope_dim=64}_NVIDIA_H200.json kernel name mla_decode_attentnion use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/mla_decode_attentnion/{out_dtype=torch.bfloat16,q_head_dim=512,q_head_num=128,q_rope_dim=64}_NVIDIA_H200.json kernel name mla_decode_attentnion use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/mla_decode_attentnion/{out_dtype=torch.bfloat16,q_head_dim=512,q_head_num=128,q_rope_dim=64}_NVIDIA_H200.json kernel name mla_decode_attentnion use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/mla_decode_attentnion/{out_dtype=torch.bfloat16,q_head_dim=512,q_head_num=128,q_rope_dim=64}_NVIDIA_H200.json kernel name mla_decode_attentnion use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/mla_decode_attentnion/{out_dtype=torch.bfloat16,q_head_dim=512,q_head_num=128,q_rope_dim=64}_NVIDIA_H200.json kernel name mla_decode_attentnion use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/mla_decode_attentnion/{out_dtype=torch.bfloat16,q_head_dim=512,q_head_num=128,q_rope_dim=64}_NVIDIA_H200.json kernel name mla_decode_attentnion use default kernel setting
WARNING 03-24 20:01:21 [kernel_config.py:40] can not find config_path /mtc/baishihao/lightllm/lightllm/common/all_kernel_configs/mla_decode_attentnion/{out_dtype=torch.bfloat16,q_head_dim=512,q_head_num=128,q_rope_dim=64}_NVIDIA_H200.json kernel name mla_decode_attentnion use default kernel setting
INFO 03-24 20:01:22 [cache_tensor_manager.py:71] pid 2498642 cuda graph alloc graph out mem (128, 129280) torch.float32 16547840 16547840
INFO 03-24 20:01:22 [cache_tensor_manager.py:73] cuda graph managed_total_tensor_bytes: 66191360
INFO 03-24 20:01:22 [cache_tensor_manager.py:71] pid 2498652 cuda graph alloc graph out mem (128, 129280) torch.float32 16547840 16547840
INFO 03-24 20:01:22 [cache_tensor_manager.py:73] cuda graph managed_total_tensor_bytes: 66191360
INFO 03-24 20:01:22 [cache_tensor_manager.py:71] pid 2498643 cuda graph alloc graph out mem (128, 129280) torch.float32 16547840 16547840
INFO 03-24 20:01:22 [cache_tensor_manager.py:73] cuda graph managed_total_tensor_bytes: 66191360
INFO 03-24 20:01:22 [cache_tensor_manager.py:71] pid 2498648 cuda graph alloc graph out mem (128, 129280) torch.float32 16547840 16547840
INFO 03-24 20:01:22 [cache_tensor_manager.py:73] cuda graph managed_total_tensor_bytes: 66191360
INFO 03-24 20:01:22 [cache_tensor_manager.py:71] pid 2498640 cuda graph alloc graph out mem (128, 129280) torch.float32 16547840 16547840
INFO 03-24 20:01:22 [cache_tensor_manager.py:73] cuda graph managed_total_tensor_bytes: 66191360
INFO 03-24 20:01:22 [cache_tensor_manager.py:71] pid 2498651 cuda graph alloc graph out mem (128, 129280) torch.float32 16547840 16547840
INFO 03-24 20:01:22 [cache_tensor_manager.py:73] cuda graph managed_total_tensor_bytes: 66191360
INFO 03-24 20:01:22 [cache_tensor_manager.py:71] pid 2498644 cuda graph alloc graph out mem (128, 129280) torch.float32 16547840 16547840
INFO 03-24 20:01:22 [cache_tensor_manager.py:73] cuda graph managed_total_tensor_bytes: 66191360
INFO 03-24 20:01:22 [cache_tensor_manager.py:71] pid 2498641 cuda graph alloc graph out mem (128, 129280) torch.float32 16547840 16547840
INFO 03-24 20:01:22 [cache_tensor_manager.py:73] cuda graph managed_total_tensor_bytes: 66191360
INFO 03-24 20:02:55 [cuda_graph.py:188] Capture cudagraph success, batch_size <=128 and max_len_in_batch <= 16384 will infer with cudagraph.
INFO 03-24 20:02:55 [basemodel.py:517] disable_check_max_len_infer is true
INFO 03-24 20:02:55 [cuda_graph.py:188] Capture cudagraph success, batch_size <=128 and max_len_in_batch <= 16384 will infer with cudagraph.
INFO 03-24 20:02:55 [basemodel.py:517] disable_check_max_len_infer is true
INFO 03-24 20:02:55 [cuda_graph.py:188] Capture cudagraph success, batch_size <=128 and max_len_in_batch <= 16384 will infer with cudagraph.
INFO 03-24 20:02:55 [basemodel.py:517] disable_check_max_len_infer is true
INFO 03-24 20:02:55 [cuda_graph.py:188] Capture cudagraph success, batch_size <=128 and max_len_in_batch <= 16384 will infer with cudagraph.
INFO 03-24 20:02:55 [basemodel.py:517] disable_check_max_len_infer is true
INFO 03-24 20:02:55 [cuda_graph.py:188] Capture cudagraph success, batch_size <=128 and max_len_in_batch <= 16384 will infer with cudagraph.
INFO 03-24 20:02:55 [basemodel.py:517] disable_check_max_len_infer is true
INFO 03-24 20:02:55 [cuda_graph.py:188] Capture cudagraph success, batch_size <=128 and max_len_in_batch <= 16384 will infer with cudagraph.
INFO 03-24 20:02:55 [basemodel.py:517] disable_check_max_len_infer is true
INFO 03-24 20:02:55 [cuda_graph.py:188] Capture cudagraph success, batch_size <=128 and max_len_in_batch <= 16384 will infer with cudagraph.
INFO 03-24 20:02:55 [basemodel.py:517] disable_check_max_len_infer is true
INFO 03-24 20:02:55 [cuda_graph.py:188] Capture cudagraph success, batch_size <=128 and max_len_in_batch <= 16384 will infer with cudagraph.
INFO 03-24 20:02:55 [basemodel.py:517] disable_check_max_len_infer is true
